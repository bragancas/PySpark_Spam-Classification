{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x3aJEGS5keqH"
   },
   "source": [
    "\n",
    "# Detecting Spam with Spark\n",
    "\n",
    "This notebook is about classification of e-mail messages as spam or non-spam in Spark. We will go through the whole process from loading and preprocessing to training and testing classifiers in a distributed way in Spark.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CdFlCqCFkeqL"
   },
   "source": [
    "## Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Braganca\n",
      "<SparkContext master=local[*] appName=pyspark-shell>\n",
      "<pyspark.sql.session.SparkSession object at 0x7f8052a29b10>\n"
     ]
    }
   ],
   "source": [
    "%cd\n",
    "!brew install openjdk@8\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
    "!tar -xvf spark-2.4.5-bin-hadoop2.7.tgz > /dev/null\n",
    "!pip install -q findspark\n",
    "import os \n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/local/Cellar/openjdk@8/1.8.0+282\" # appropriately set environment variables\n",
    "os.environ[\"SPARK_HOME\"] = \"/Users/Braganca/spark-2.4.5-bin-hadoop2.7\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "# get a spark context\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "print(sc)\n",
    "# get the context\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "nncrHFdwqUmE",
    "outputId": "6f86dee2-dda3-407f-9c0b-83827e8c83e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Braganca/lingspam_public\n",
      "This directory contains the Ling-Spam corpus, as described in the \r\n",
      "paper:\r\n",
      "\r\n",
      "I. Androutsopoulos, J. Koutsias, K.V. Chandrinos, George Paliouras, \r\n",
      "and C.D. Spyropoulos, \"An Evaluation of Naive Bayesian Anti-Spam \r\n",
      "Filtering\". In Potamias, G., Moustakis, V. and van Someren, M. (Eds.), \r\n",
      "Proceedings of the Workshop on Machine Learning in the New Information \r\n",
      "Age, 11th European Conference on Machine Learning (ECML 2000), \r\n",
      "Barcelona, Spain, pp. 9-17, 2000.\r\n",
      "\r\n",
      "There are four subdirectories, corresponding to four versions of \r\n",
      "the corpus:\r\n",
      "\r\n",
      "bare: Lemmatiser disabled, stop-list disabled.\r\n",
      "lemm: Lemmatiser enabled, stop-list disabled.\r\n",
      "lemm_stop: Lemmatiser enabled, stop-list enabled.\r\n",
      "stop: Lemmatiser disabled, stop-list enabled.\r\n",
      "\r\n",
      "Each one of these 4 directories contains 10 subdirectories (part1, \r\n",
      "..., part10). These correspond to the 10 partitions of the corpus \r\n",
      "that were used in the 10-fold experiments. In each repetition, one \r\n",
      "part was reserved for testing and the other 9 were used for training. \r\n",
      "\r\n",
      "Each one of the 10 subdirectories contains both spam and legitimate \r\n",
      "messages, one message in each file. Files whose names have the form\r\n",
      "spmsg*.txt are spam messages. All other files are legitimate messages.\r\n",
      "\r\n",
      "By obtaining a copy of this corpus you agree to acknowledge the use \r\n",
      "and origin of the corpus in any published work of yours that makes \r\n",
      "use of the corpus, and to notify the person below about this work.\r\n",
      "\r\n",
      "Ion Androutsopoulos \r\n",
      "http://www.aueb.gr/users/ion/\r\n",
      "Ling-Spam corpus last updated: July 17, 2000\r\n",
      "This file (readme.txt) last updated: July 30, 2003.\r\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/Braganca/lingspam_public\n",
    "\n",
    "!cat readme.txt # shows the content of the readme file, which explains the structure of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nb02XcMOkeqq"
   },
   "source": [
    "## Read the dataset and create RDDs \n",
    "\n",
    "Load all text files per directory (`part1, part2, ... , part10`) using `wholeTextFiles()`, which creates one RDD per part, containing tuples `(filename, text)`.\n",
    "Use one of the RDDs as test set, the rest as training set. For the training set create the union of the remaining RDDs. If the filename starts with 'spmsg' it is spam, otherwise it is not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "H7iF1lZukeqt",
    "outputId": "42e273c3-3f0a-4932-a745-ac4e4c2bc648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Braganca/lingspam_public\n",
      "\u001b[34mbare\u001b[m\u001b[m       \u001b[34mlemm\u001b[m\u001b[m       \u001b[34mlemm_stop\u001b[m\u001b[m  readme.txt \u001b[34mstop\u001b[m\u001b[m\n",
      "creating RDDs\n",
      "/Users/Braganca/lingspam_public/bare/part3\n",
      "/Users/Braganca/lingspam_public/bare/part4\n",
      "/Users/Braganca/lingspam_public/bare/part5\n",
      "/Users/Braganca/lingspam_public/bare/part2\n",
      "/Users/Braganca/lingspam_public/bare/part10\n",
      "/Users/Braganca/lingspam_public/bare/part9\n",
      "/Users/Braganca/lingspam_public/bare/part7\n",
      "/Users/Braganca/lingspam_public/bare/part1\n",
      "/Users/Braganca/lingspam_public/bare/part6\n",
      "/Users/Braganca/lingspam_public/bare/part8\n",
      "creating RDD union\n",
      "created the RDDs\n",
      "testRDD.count():  289\n",
      "testRDD.getNumPartitions(): 2\n",
      "testRDD.getStorageLevel(): Serialized 1x Replicated\n",
      "testRDD.take(1):  [('8-1064msg1', 'Subject: re : 8 . 1044 , disc : grammar in schools\\n\\n( re message from : linguist @ linguistlist . org ) > > linguist list : vol-8 - 1044 . sat jul 12 1997 . issn : 1068-4875 . > > subject : 8 . 1044 , disc : grammar in schools > > i know and teach that not all infinitives contain ` to \\' . i also give > the students examples ( e . g . ` i asked him to kindly apologise \\' ) where > placing the adverb anywhere else would cause ambiguity . > > jennifer chew an example i once concocted to justify \" splitting the infintive \" ( or not , as the case may be ) is : a ) after a heavy meal , i prepared slowly to go home digesting b ) after a heavy meal , i prepared to slowly go home digesting c ) after a heavy meal , i prepared to go home slowly digesting in this context , with the possible exception of the third case , the natural ( and therefore near-enough unambiguous ) association of the adverb is as follows : a ) after a heavy meal , i prepared _ slowly to go home digesting b ) after a heavy meal , i prepared to slowly _ go _ home digesting c ) after a heavy meal , i prepared to go home slowly _ digesting ( this was long ago , when you got glared at for splitting an infinitive regardless of whether it was the only place to put the adverb so as to express what you meant and not something else : this example achieved , as nearly as i could , three quite distinct and natural meanings for \" . . . slowly to go home . . . \" , \" . . . to slowly go home . . . \" and \" . . . to go home slowly . . . \" . i \\' m not 100 per cent happy with it , for obvious reasons , and it would be interesting to see if anyone can come up with a better , more clear-cut one ) . [ and , to really \" epater les bourgeois \" , i reckon you could even make a case for \" . . . i prepared to , slowly , go home digesting \" : the implication being that the meal was so very heavy that the walk home should be correpondingly delicate , as emphasised by the pause in rhythm marked by the commas ] . ted . ( ted . harding @ nessie . mcc . ac . uk )\\n')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def makeTestTrainRDDs(pathString):\n",
    "    \"\"\" Takes one of the four subdirectories of the lingspam dataset and returns two RDDs one each for testing and training. \"\"\"\n",
    "    # We should see 10 parts that we can use for creating train and test sets.\n",
    "    p = Path(pathString) # gets a path object representing the current directory path.\n",
    "    dirs = list(p.iterdir()) # get the directories part1 ... part10. \n",
    "    #print(dirs) \n",
    "    rddList = []\n",
    "    # create an RDD for each 'part' directory and add them to rddList\n",
    "    print('creating RDDs')\n",
    "    for d in dirs: # iterate through the directories\n",
    "        dir_path = str(d.resolve())\n",
    "        print(dir_path) \n",
    "        rdd = sc.wholeTextFiles(dir_path)\n",
    "        rddList.append(rdd)\n",
    "    #print('len(rddList)', len(rddList))  # should have 10 RDDs in the list # just for testing\n",
    "    #print(rddList[1].take(1)) # just for testing\n",
    "\n",
    "    testRDD1 = rddList[9] # set the test set\n",
    "    trainRDD1 = rddList[0] # start the training set from 0 and \n",
    "    # create a union of the remaining RDDs (parts 0-8)\n",
    "    print('creating RDD union')\n",
    "    for i in range(1, 9):\n",
    "        trainRDD1 = trainRDD1.union(rddList[i]) \n",
    "    # both RDDs should remove the paths and extensions from the filename. \n",
    "    testRDD2 = testRDD1.map(lambda fn_txt: (re.split('[/\\.]', fn_txt[0])[-2],fn_txt[1]))\n",
    "    trainRDD2 = trainRDD1.map(lambda fn_txt: (re.split('[/\\.]', fn_txt[0])[-2],fn_txt[1]))\n",
    "    return (trainRDD2, testRDD2)\n",
    "\n",
    "# make sure we are in the right directory\n",
    "%cd /Users/Braganca/lingspam_public \n",
    "# this should show the directories \"bare  lemm  lemm_stop  readme.txt  stop\"\n",
    "!ls \n",
    "# the code below is for testing the function makeTestTrainRDDs\n",
    "trainRDD_testRDD = makeTestTrainRDDs('bare') # read from the 'bare' directory - this takes a bit of time\n",
    "(trainRDD, testRDD) = trainRDD_testRDD # unpack the returned tuple\n",
    "print('created the RDDs') \n",
    "print('testRDD.count(): ', testRDD.count()) \n",
    "#print('trainRDD.count(): ', trainRDD.count()) # commented out to save time as it takes some time to create RDD from all the files\n",
    "print('testRDD.getNumPartitions():', testRDD.getNumPartitions()) \n",
    "print('testRDD.getStorageLevel():', testRDD.getStorageLevel()) # Serialized, 1x Replicated, expected to be (False, False, False, False, 1) \n",
    "print('testRDD.take(1): ', testRDD.take(1)) # should be (filename, message) \n",
    "rdd1 = testRDD # to be used later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUKltxq5keqw"
   },
   "source": [
    "## Tokenize and remove punctuation\n",
    "\n",
    "Now we need to split the words(*tokenization*) and remove punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "AB_nfmhYkeqx",
    "outputId": "f9774c04-6da9-4a2a-a48a-04e13c33df79",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('8-1064msg1', ['Subject', ':', 're', ':', '8', '1044', 'disc', ':', 'grammar', 'in', 'schools', 're', 'message', 'from', ':', 'linguist', '@', 'linguistlist', 'org', '>', '>', 'linguist', 'list', ':', 'vol-8', '-', '1044', 'sat', 'jul', '12', '1997', 'issn', ':', '1068-4875', '>', '>', 'subject', ':', '8', '1044', 'disc', ':', 'grammar', 'in', 'schools', '>', '>', 'i', 'know', 'and', 'teach', 'that', 'not', 'all', 'infinitives', 'contain', '`', 'to', \"'\", 'i', 'also', 'give', '>', 'the', 'students', 'examples', 'e', 'g', '`', 'i', 'asked', 'him', 'to', 'kindly', 'apologise', \"'\", 'where', '>', 'placing', 'the', 'adverb', 'anywhere', 'else', 'would', 'cause', 'ambiguity', '>', '>', 'jennifer', 'chew', 'an', 'example', 'i', 'once', 'concocted', 'to', 'justify', '``', 'splitting', 'the', 'infintive', '``', 'or', 'not', 'as', 'the', 'case', 'may', 'be', 'is', ':', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'in', 'this', 'context', 'with', 'the', 'possible', 'exception', 'of', 'the', 'third', 'case', 'the', 'natural', 'and', 'therefore', 'near-enough', 'unambiguous', 'association', 'of', 'the', 'adverb', 'is', 'as', 'follows', ':', 'a', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'slowly', 'to', 'go', 'home', 'digesting', 'b', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', 'c', 'after', 'a', 'heavy', 'meal', 'i', 'prepared', 'to', 'go', 'home', 'slowly', 'digesting', 'this', 'was', 'long', 'ago', 'when', 'you', 'got', 'glared', 'at', 'for', 'splitting', 'an', 'infinitive', 'regardless', 'of', 'whether', 'it', 'was', 'the', 'only', 'place', 'to', 'put', 'the', 'adverb', 'so', 'as', 'to', 'express', 'what', 'you', 'meant', 'and', 'not', 'something', 'else', ':', 'this', 'example', 'achieved', 'as', 'nearly', 'as', 'i', 'could', 'three', 'quite', 'distinct', 'and', 'natural', 'meanings', 'for', '``', 'slowly', 'to', 'go', 'home', '``', '``', 'to', 'slowly', 'go', 'home', '``', 'and', '``', 'to', 'go', 'home', 'slowly', '``', 'i', \"'\", 'm', 'not', '100', 'per', 'cent', 'happy', 'with', 'it', 'for', 'obvious', 'reasons', 'and', 'it', 'would', 'be', 'interesting', 'to', 'see', 'if', 'anyone', 'can', 'come', 'up', 'with', 'a', 'better', 'more', 'clear-cut', 'one', 'and', 'to', 'really', '``', 'epater', 'les', 'bourgeois', '``', 'i', 'reckon', 'you', 'could', 'even', 'make', 'a', 'case', 'for', '``', 'i', 'prepared', 'to', 'slowly', 'go', 'home', 'digesting', '``', ':', 'the', 'implication', 'being', 'that', 'the', 'meal', 'was', 'so', 'very', 'heavy', 'that', 'the', 'walk', 'home', 'should', 'be', 'correpondingly', 'delicate', 'as', 'emphasised', 'by', 'the', 'pause', 'in', 'rhythm', 'marked', 'by', 'the', 'commas', 'ted', 'ted', 'harding', '@', 'nessie', 'mcc', 'ac', 'uk']), ('6-806msg1', ['Subject', ':', 'swadesh', 'list', 'does', 'anyone', 'have', 'a', 'copy', 'of', 'the', 'swadesh', 'word', 'list', 'at', 'hand', 'i', 'should', 'like', 'to', 'get', 'a', 'copy', 'by', 'email', 'as', 'soon', 'as', 'is', 'practicable', 'thanks', 'in', 'advance', 'adams', 'bodomo', 'bodomo', '@', 'csli', 'stanford', 'edu']), ('6-816msg1', ['Subject', ':', 'summary', ':', 'half', 'a', 'day', 'dear', 'readers', 'many', 'thanks', 'to', 'all', 'respondents', '51', 'who', 'sent', 'replies', 'to', 'my', 'query', 'about', 'time', 'phrases', 'and', 'native-speaker', 'judgments', 'there', 'were', 'too', 'many', 'replies', 'to', 'acknowledge', 'individually', 'many', 'of', 'you', 'asked', 'for', 'a', 'summary', 'so', 'here', 'goes', '#', '#', '=', 'strongly', 'preferred', '#', '=', 'good', '=', 'awkward', 'x', '=', 'bad', 'yuck', 'the', 'figures', 'following', 'each', 'phrase', 'give', 'the', 'number', 'of', 'votes', 'cast', 'in', 'each', 'category', 'they', 'do', \"n't\", 'all', 'add', 'up', 'to', '51', 'because', 'some', 'respondents', 'rs', 'expressed', 'only', 'their', 'first', 'preference', '#', '#', '#', 'x', '1', 'the', 'family', 'spent', 'in', 'ipswich', 'a', 'a', 'day', 'and', 'a', 'half', '24', '26', '0', '0', 'b', 'one', 'and', 'a', 'half', 'days', '1', '31', '7', '7', 'c', 'thirty-six', 'hours', '1', '29', '6', '9', 'this', 'makes', 'a', 'a', 'clear', 'winner', 'but', 'for', 'many', 'rs', 'b', 'and', 'c', 'are', 'acceptable', 'too', 'depending', 'on', 'context', 'comments', ':', 'a', 'implies', 'enough', 'time', 'to', 'socialize', 'implies', 'most', 'of', 'sunday', 'spent', 'in', 'ipswich', 'leaving', 'at', 'noon', 'on', 'monday', 'b', 'implies', 'ipswich', 'was', 'part', 'of', 'a', 'series', 'of', 'visits', 'implies', 'an', 'overnight', 'stay', 'part', 'of', 'a', 'list', 'stilted', 'c', 'military', '/', 'aeronautical', 'whirlwind', 'tour', 'every', 'moment', 'packed', 'with', 'feverish', 'activity', 'at', 'a', 'conference', '-', 'one', 'and', 'a', 'half', 'days', 'spent', 'on', 'syntax', 'working', 'under', 'time', 'pressure', 'airplane', 'layover', '/', 'waystation', 'ok', 'for', 'negative', 'experiences', 'flu', '/', 'jail', 'i', 'take', 'it', 'a', \"'\", 'layover', \"'\", 'is', 'us', 'for', 'british', \"'s\", 'topover', \"'\", 'or', 'maybe', 'it', \"'s\", 'a', 'stopover', 'with', 'a', 'visit', 'to', 'a', 'girlfriend', '2', 'it', 'took', 'me', 'to', 'write', 'the', 'book', 'a', 'six', 'months', '16', '34', '0', '0', 'b', 'a', 'half-year', '0', '10', '9', '25', 'c', 'half', 'a', 'year', '3', '44', '1', '1', 'while', 'there', 'is', 'a', 'preference', 'for', 'a', 'c', 'is', 'not', 'far', 'behind', 'b', 'is', 'problematic', 'comments', ':', 'a', 'feels', 'shorter', 'not', 'as', 'much', 'effort', 'required', 'as', \"'\", 'half', 'a', 'year', \"'\", 'b', 'suggests', 'an', 'academic', 'half-year', 'yuck', 'unnatural', 'sounds', 'non-native', 'or', 'british', 'from', 'american', 'rs', 'sounds', 'american', 'from', 'british', 'rs', 'ok', 'in', 'financial', 'contexts', '-', 'a', 'half', 'year', 'is', 'either', 'the', 'first', 'or', 'second', 'half', 'not', 'an', 'arbitrary', '6', 'month', 'period', 'c', 'sounds', 'longer', 'than', 'six', 'months', 'emphatic', 'with', 'stress', 'on', \"'\", 'year', \"'\", '-', '3', 'we', \"'\", 'll', 'be', 'leaving', 'in', 'a', 'half', 'an', 'hour', '6', '44', '0', '0', 'b', 'a', 'half-hour', '2', '24', '10', '11', 'a', 'wins', 'b', 'splits', 'people', 'into', 'two', 'roughly', 'equal', 'camps', 'comments', ':', 'b', 'sounds', 'formal', 'awkward', 'funny', 'sounds', 'normal', '-', 'we', \"'\", 'll', 'be', 'leaving', 'inna', 'haf', 'our', 'from', 'now', '4', 'tom', 'worked', 'for', 'in', 'a', 'lab', 'a', 'a', 'year', 'and', 'a', 'half', '13', '37', '1', '0', 'b', 'one', 'and', 'a', 'half', 'years', '0', '32', '6', '7', 'c', 'eighteen', 'months', '3', '39', '6', '0', 'while', 'a', 'wins', 'the', 'other', 'two', 'are', \"n't\", 'so', 'far', 'behind', 'comments', ':', 'a', 'least', 'exact', 'b', 'more', 'exact', 'part', 'of', 'calculation', 'eg', 'for', 'pension', 'stilted', 'fussy', 'c', 'most', 'exact', 'emphasizes', 'duration', 'ok', 'for', 'children', \"'s\", 'ages', 'upto', 'two', 'years', 'ok', 'in', 'contexts', 'where', 'precision', 'is', 'required', 'suggests', 'tom', 'was', 'less', 'involved', 'in', 'the', 'job', 'maybe', 'a', 'temporary', 'job', 'using', 'months', 'for', 'time', 'greater', 'than', 'a', 'year', 'and', 'hours', 'for', 'time', 'greater', 'than', 'a', 'day', 'makes', 'the', 'time', 'seem', 'more', 'rushed', 'half', 'an', 'inch', 'is', 'ok', 'but', 'not', 'half', 'a', 'foot', 'but', 'half', 'a', 'yard', 'is', 'ok', 'if', 'bying', 'cloth', 'even', 'though', 'we', 'do', \"n't\", 'normally', 'speak', 'of', 'half', 'a', 'yard', 'the', 'overall', 'impression', 'that', 'i', 'get', 'is', 'that', 'context', 'and', 'pragmatic', 'considerations', 'determine', 'which', 'lexical', 'item', 'will', 'be', 'acceptable', 'in', 'any', 'given', 'slot', 'and', 'even', 'then', 'there', 'is', 'considerably', 'more', 'tolerance', 'for', 'some', 'expressions', 'than', 'i', 'would', 'have', 'thought', 'possible', '/', 'probable', 'three', 'rs', 'said', 'all', 'eleven', 'phrases', 'are', '100', '%', 'ok', 'i', 'have', \"n't\", 'given', 'details', 'of', 'rs', \"'\", 'background', '/', 'nationality', 'etc', 'since', 'not', 'all', 'rs', 'gave', 'me', 'details', 'however', 'about', 'three-quarters', 'of', 'replies', 'came', 'from', 'the', 'usa', 'i', 'hope', 'this', 'has', 'been', 'of', 'some', 'interest', 'many', 'thanks', 'for', 'your', 'response', 'roger', 'maylor', 'dept', 'of', 'linguistics', 'and', 'english', 'language', 'university', 'of', 'durham', 'uk']), ('8-1208msg1', ['Subject', ':', 'summary', ':', 'double', '-', 'dutch', 'and', 'youthese', '/', 'pig', 'latin', 'my', 'original', 'query', 'was', 'posted', 'on', 'jul', '12', '1997', 'in', 'linguist', 're', ':', '8', '1048', 'and', 'asked', 'for', 'data', 'on', ':', 'a', 'secret', 'signalization', 'codes', 'among', 'children', 'approaching', 'but', 'still', 'not', 'having', 'fully', 'reached', 'the', 'age', 'of', 'adolescence', 'particularly', 'so-called', '``', 'double', '-', 'dutch', '``', 'a', 'more', 'or', 'less', 'invariant', 'standard', 'syllable', 'is', 'inserted', 'into', 'every', 'word', 'to', 'render', 'it', 'unrecognizable', 'in', 'various', 'languages', 'of', 'the', 'world', 'b', 'exclusivist', 'but', 'not', 'particularly', 'secretive', 'youth-specific', 'slang', 'so-called', '``', 'youthese', '``', 'among', 'teenagers', 'adolescents', 'functioning', 'as', 'peer', 'in-group', 'or', 'clique', 'trademark', 'i', 'have', 'received', 'a', 'great', 'deal', 'of', 'very', 'useful', 'information', 'i', 'have', 'not', 'attempted', 'making', 'a', 'summary', 'earlier', 'because', 'new', 'responses', 'kept', 'coming', 'in', 'i', 'suppose', 'i', 'chose', 'an', 'inopportune', 'time', 'to', 'send', 'in', 'my', 'query', 'when', 'most', 'people', 'are', 'on', 'vacation', '/', 'holidays', 'having', 'now', 'also', 'recieved', 'the', 'material', 'one', 'respondent', 'said', 'she', 'would', 'send', 'me', 'after', 'returning', 'from', 'a', 'journey', 'i', 'can', 'now', 'proceed', 'with', 'the', 'summary', ':', 'the', 'responses', 'also', 'included', 'new', 'leads', 'to', 'further', 'search', 'and', 'the', 'following', 'is', 'a', 'total', 'summary', 'i', 'first', 'of', 'all', 'want', 'to', 'thank', 'all', 'the', 'responders', 'and', 'contributors', 'for', 'their', 'bery', 'helpful', 'and', 'informative', 'messages', ':', 'jannis', 'k', 'androutsopoulos', '<', 'androuts', '@', 'novell1', 'gs', 'uni-heidelberg', 'de', '>', 'jack', 'aubert', '<', 'jaubert', '@', 'cpcug', 'org', '>', 'rick', 'mc', 'callister', '<', 'rmccalli', '@', 'muw', 'edu', '>', 'bill', 'fisher', '<', 'william', 'fisher', '@', 'nist', 'gov', '>', 'tim', 'jake', 'gluckman', '<', 'tjgluckman', '@', 'aol', 'com', '>', 'jack', 'hall', '<', 'jhall', '@', 'uh', 'edu', '>', 'marion', 'kee', '<', 'marion', 'kee', '@', 'cs', 'cmu', 'edu', '>', 'nobuko', 'koyama', '-', 'murakami', '<', 'koyamamu', '@', 'hawaii', 'edu', '>', 'nathan', 'sanders', '<', 'sanders', '@', 'ling', 'ucsc', 'edu', '>', 'nik', 'taylor', '<', 'jnataylor', '@', 'pcola', 'gulf', 'net', '>', 'markell', 'r', 'west', '<', 'markell', '@', 'afterlife', 'ncsc', 'mil', '>', 'mark', 'a', 'wilson', '<', 'maw', '@', 'annap', 'infi', 'net', '>', 'sorry', 'if', 'i', 'missed', 'somebody', '1', 'first', 'the', 'direct', 'respondents', 'to', 'my', 'original', 'query', ':', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'rick', 'mc', 'callister', ':', 'called', 'my', 'attention', 'to', 'the', 'fact', 'the', 'pig', 'latin', 'was', 'the', 'term', 'more', 'commonly', 'used', 'for', 'what', 'i', 'called', '``', 'double', 'dutch', '``', 'and', 'also', 'gave', 'me', 'the', 'url', 'of', 'his', 'www', 'spanish', 'pig', 'latin', 'page', ':', 'http', ':', '/', '/', 'www', 'muw', 'edu', '/', '~', 'rmccalli', '/', 'spigpayatinlay', 'html', 'the', 'www', 'page', 'is', 'very', 'informative', 'it', 'also', 'suggested', 'a', 'new', 'venue', 'of', 'search', 'which', 'proved', 'quite', 'fruitful', 'i', 'e', 'i', 'started', 'to', 'search', 'the', 'internet', 'for', 'mentionings', 'of', '``', 'pig', 'latin', '``', 'see', '2', 'below', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'bill', 'fisher', ':', 'one', 'good', 'example', 'of', 'this', 'which', 'you', 'may', 'already', 'be', 'aware', 'of', 'is', '``', 'boontling', '``', 'a', 'jargon', 'that', 'was', 'developed', 'in', 'the', '19th', 'century', 'in', 'marin', 'county', 'california', 'i', \"'\", 've', 'got', 'a', 'pretty', 'decent', 'book', 'on', 'it', '``', 'boontling', 'an', 'american', 'lingo', '``', 'by', 'charles', 'c', 'adams', 'u', 'of', 'texas', 'press', 'austin', '1971', 'isbn', '0-292', '-', '70082', '-', '2', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'tim', 'gluckman', ':', 'when', 'i', 'was', 'at', 'school', 'in', 'the', \"'\", '60s', '-', 'in', 'stockport', 'england', '-', 'i', 'recall', 'that', 'oneschoolgirl', 'clique', 'in', 'my', 'year', 'spoke', 'one', 'of', 'these', 'insider', 'languages', 'one', 'day', 'i', 'asked', 'one', '-', 'they', 'were', 'all', 'in', 'my', 'schoolyear', '-', 'of', 'the', 'more', 'ansprechbar', 'of', 'these', 'recently', 'pubertied', 'schulmaedchen', 'what', 'they', 'were', 'saying', 'as', 'far', 'as', 'i', 'can', 'recall', 'it', 'her', 'explanation', 'was', 'that', 'their', 'geheimsprache', 'included', 'a', 'variable', 'substitution', 'of', 't', '/', 'd', '-', 'perhaps', 'other', 'consonants', 'too', '-', 'before', 'the', 'end', 'of', 'the', 'word', 'it', 'certainly', 'had', 'the', 'affect', 'of', 'of', 'rendering', 'their', 'conversations', 'incomprehensible', 'this', 'is', 'the', 'only', 'time', 'i', 'ever', 'came', 'across', 'it', 'c', '1964', '/', '5', 'they', 'spoke', 'it', 'for', '6', 'months', 'as', 'far', 'as', 'i', 'can', 'recall', 'whether', 'it', 'went', 'on', 'beyond', 'that', 'i', 'do', \"n't\", 'know', 'these', 'girls', 'were', 'in', 'the', 'middle', 'of', 'three', 'streams', 'at', 'the', 'grammar', 'school', '15', 'kilometres', 'south', 'of', 'manchester', 'where', 'i', 'went', 'to', 'at', 'that', 'time', 'and', 'on', 'a', 'question', 'of', 'mine', 'indicated', 'they', 'were', 'at', 'an', 'age', 'where', 'they', 'were', 'actively', 'dating', 'with', 'boys', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'marion', 'kee', ':', 'there', 'was', 'a', 'discussion', 'on', 'linguist', 'i', 'think', 'sometime', 'in', '1995', 'about', 'pig', 'latin', 'and', 'related', 'topics', 'i', 'think', 'there', 'were', 'examples', 'cited', 'from', 'a', 'number', 'of', 'different', 'languages', 'the', 'discussion', 'might', 'have', 'included', 'a', 'list', 'of', 'references', 'to', 'find', 'it', 'in', 'the', 'linguist', 'archive', 'try', 'searching', 'on', '``', 'pig', 'latin', '``', 'and', '/', 'or', '``', 'egg', 'latin', '``', 'in', 'egg', 'latin', 'every', 'syllable', 'gets', 'the', 'syllable', '``', 'egg', '``', 'added', 'prior', 'to', 'its', 'vowel', 'e', 'g', '``', 'eggegg', 'leggateggin', '``', '-', '-', '``', 'egg', 'latin', '``', 'english', 'only', 'as', 'far', 'as', 'i', 'know', 'and', 'my', 'ex-husband', 'learned', 'it', 'when', 'he', 'was', '10', 'or', '11', 'in', 'athens', 'ohio', 'usa', 'this', 'suggestion', 'too', 'opened', 'a', 'fruitful', 'venue', 'for', 'further', 'search', 'see', '3', 'below', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'nik', 'taylor', ':', 'my', 'cousins', 'my', 'brother', 'and', 'i', 'had', 'a', 'code', 'called', 'flip-top', 'you', 'started', 'out', 'by', 'flipping', 'around', 'pairs', 'of', 'letters', 'double', 'letters', 'being', 'counted', 'as', 'one', 'and', 'adding', '-', 'ot', 'to', 'consonants', 'and', 'nothing', 'to', 'vowels', 'doubles', 'being', 'indicated', 'by', '``', 'squared', '``', 'so', '``', 'hello', '``', '-', '``', 'e', 'hot', 'o', 'lot-squared', '``', 'she', 'and', 'her', 'friend', 'had', 'invented', 'it', 'as', '``', 'tot', '``', 'i', 'think', 'that', 'was', 'its', 'name', 'and', 'it', 'was', 'just', 'adding', '-', 'ot', 'to', 'consonants', 'and', 'the', '``', 'squared', '``', 'part', 'so', '``', 'hello', '``', '-', '``', 'hote', 'lot-squared', 'o', '``', 'i', 'added', 'the', 'flipping', 'part', '2', 'rick', 'mc', 'callister', \"'s\", 'www', 'page', 'suggested', 'a', 'search', 'for', 'other', 'such', 'pages', 'but', 'i', 'only', 'found', 'one', 'that', 'of', 'nathan', 'sanders', ':', 'http', ':', '/', '/', 'ling', 'ucsc', 'edu', '/', '~', 'sanders', '/', 'research', 'html', 'which', 'also', 'was', 'very', 'informative', 'on', 'language', 'games', 'referred', 'to', 'as', 'ludlings', 'but', 'i', 'wrote', 'the', 'owner', 'and', 'got', 'further', 'information', ':', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'nathan', 'sanders', ':', 'a', 'good', 'place', 'to', 'start', 'would', 'be', 'the', 'work', 'of', 'bruce', 'bagemihl', 'who', 'has', 'done', 'a', 'lot', 'of', 'work', 'in', 'the', 'area', 'of', 'ludlings', '/', 'language-games', 'here', 'are', 'two', 'references', 'on', 'ludlings', 'the', 'first', 'has', 'a', 'large', 'list', 'of', 'examples', 'while', 'the', 'second', 'has', 'more', 'explanation', 'of', 'the', 'ludling', 'phenomenon', 'itself', ':', 'bagemihl', 'bruce', '1989', '`', '`', 'the', 'crossing', 'constraint', 'and', 'backwards', 'languages', \"'\", \"'\", 'natural', 'language', 'and', 'linguistic', 'theory', 'vol', '7', 'pp', '481-549', 'bagemihl', 'bruce', '1996', '`', '`', 'language', 'games', 'and', 'related', 'areas', \"'\", \"'\", 'in', 'john', 'a', 'goldsmith', 'ed', 'the', 'handbook', 'of', 'phonological', 'theory', 'cambridge', ':', 'blackwell', 'publishers', 'pp', '697-712', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '3', 'marion', 'kee', \"'s\", 'suggestion', 'to', 'search', 'the', 'linguist', 'archive', 'led', 'me', 'to', 'two', 'unsummarized', 'queries', 'so', 'i', 'mailed', 'the', 'querists', ':', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'jannis', 'androutsopoulos', ':', 'snailed-mailed', 'me', 'copious', 'material', 'on', 'a', 'colloquiium', 'she', 'organized', 'in', 'heidelberg', 'dedicated', 'to', 'questions', 'of', 'youth', 'slang', ':', 'international', 'colloquium', '``', 'linguistic', 'and', 'sociolinguistic', 'aspects', 'of', 'youth', '-', 'specific', 'language', '``', 'heidelberg', 'june', '5', '-', '7', '1997', 'hosted', 'by', 'the', 'graduiertenkolleg', '``', 'dynamics', 'of', 'non', '-', 'standard', 'varieties', '``', 'univ', 'of', 'heidelberg', '&', 'univ', 'of', 'mannheim', 'it', 'is', 'obviously', 'impossible', 'to', 'summarize', 'the', 'great', 'amount', 'of', 'data', 'in', 'the', 'space', 'available', 'here', 'so', 'i', \"'\", 'll', 'just', 'say', 'that', 'it', 'covers', 'various', 'aspects', 'of', 'youth', 'slang', 'in', 'germany', 'also', 'ex', '-', 'gdr', 'specific', 'italy', 'france', 'switzerland', 'also', 'at', 'turn', 'of', '19th', 'to', '20th', 'century', 'swedish', 'some', 'of', 'the', 'papers', 'touch', '=', 'ed', 'upon', 'influence', 'of', 'rap', '/', 'hip', '-', 'hop', 'etc', 'on', 'youth', 'slang', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'markell', 'west', ':', 'responded', 'first', 'of', 'all', 'by', 'posting', 'a', 'summary', 'of', 'responses', 'to', 'his', 'query', 'which', 'in', 'itself', 'was', 'very', 'informative', 're', ':', '8', '1079', 'apart', 'from', 'that', 'it', 'contained', 'a', 'list', 'of', 'respondents', ':', '*', '*', '*', '*', '*', '4', 'i', 'mailed', 'the', 'respondents', 'directly', 'and', 'this', 'brought', 'me', 'further', 'helpful', 'responses', ':', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'jack', 'aubert', ':', 'the', 'french', '``', 'verlan', '``', 'reverses', 'the', 'order', 'of', 'syllables', '``', 'e', 'l', \"'\", 'envers', '``', 'means', '``', 'backwards', '``', 'and', 'if', 'you', 'pronounce', 'l', \"'\", 'envers', 'with', 'its', 'syllables', 'reversed', 'you', 'get', '``', 'verlan', '``', 'this', 'is', 'definitely', 'an', 'example', 'of', 'what', 'you', 'described', 'as', 'type', 'b', '-', '-', 'adolescent', 'exclusivist', 'i', 'have', 'heard', 'it', 'said', 'that', 'verlan', 'originally', 'was', 'used', 'by', 'thieves', 'and', 'pickpockets', 'but', 'suspect', 'this', 'is', 'just', 'a', 'made-up', 'explanation', 'with', 'no', 'particular', 'basis', 'in', 'fact', 'but', 'whatever', 'its', 'origins', 'it', 'is', 'now', 'used', 'by', 'adolescents', 'as', 'an', 'exclusivist', 'slang', 'i', 'do', \"n't\", 'think', 'any', 'body', 'actually', 'uses', 'verlan', 'for', 'full', 'sentences', 'or', 'extended', 'conversations', 'it', 'mostly', 'forms', 'the', 'basis', 'for', 'individual', 'slang', 'words', 'that', 'go', 'into', 'normal', 'sentances', 'you', 'could', 'refer', 'to', 'your', 'zon-mai', 'maison', 'or', 'zon-blou', 'blou', 'son', 'there', 'was', 'a', 'movie', 'a', 'few', 'years', 'ago', 'called', '``', 'les', 'ripoux', '``', 'which', 'is', 'verlan', 'for', '``', 'les', 'pourris', '``', 'which', 'in', 'context', 'referred', 'to', 'corrupt', 'cops', 'i', 'think', 'the', 'term', 'for', 'french', '-', 'born', 'arabs', '``', 'beurs', '``', 'was', 'formed', 'using', 'some', 'version', 'of', 'verlan', 'which', 'is', 'not', 'always', 'regular', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'jack', 'hall', ':', 'in', 'my', 'response', 'to', 'the', 'query', 'about', 'pig', 'latin', 'i', 'mentioned', 'what', 'i', 'called', 'the', '``', 'op', '``', 'language', 'which', 'i', 'read', 'about', 'in', 'a', 'book', 'or', 'magazine', 'when', 'i', 'was', 'about', '10-12', 'years', 'old', 'mid', '1950', \"'s\", 'as', 'i', 'recall', 'the', 'simple', 'rule', 'was', ':', 'put', '``', 'op', '``', 'phonetically', 'a', ':', 'p', 'after', 'every', 'consonant', 'in', 'a', 'word', 'except', 'the', 'last', 'final', 'consonant', 'i', 'am', 'not', 'certain', 'what', 'the', 'rule', 'was', 'about', 'consonant', 'clusters', 'thus', '``', 'dog', '``', 'would', 'be', '``', 'dopog', '``', 'i', 'remember', 'specifically', 'that', 'the', 'word', '``', 'umbrella', '``', 'was', 'given', 'as', ':', '``', 'umopbopropellopa', '``', 'indicating', 'that', '``', 'op', '``', 'is', 'to', 'be', 'placed', 'after', 'all', 'three', 'consonants', 'at', 'the', 'beginning', 'umbr', '-', '-', 'but', 'only', 'one', 'after', 'the', 'double', '``', 'l', '``', 'i', 'have', 'never', 'met', 'anybody', 'who', 'has', 'heard', 'of', 'this', 'language', 'or', 'knew', 'how', 'to', 'use', 'it', 'and', 'since', 'i', 'learned', 'about', 'it', 'from', 'a', 'book', 'rather', 'than', 'from', 'other', 'people', 'children', 'i', 'can', 'not', 'say', 'anything', 'about', 'the', 'sociolinguistics', 'of', 'it', 'for', 'me', 'it', 'is', 'an', 'idiolect', 'we', \"'\", 're', 'talking', 'at', 'least', '40', 'years', 'here', 'but', 'the', 'strange', 'thing', 'is', 'i', 'can', 'actually', 'visualize', 'the', 'item', 'that', 'i', 'read', 'and', 'the', 'page', 'on', 'which', 'it', 'was', 'printed', 'although', 'i', 'certainly', 'do', \"n't\", 'know', 'the', 'title', 'of', 'the', 'book', 'i', 'am', 'sure', 'that', 'it', 'was', 'written', 'for', 'people', 'my', 'our', 'age', 'at', 'the', 'time', 'not', 'for', 'adults', 'i', 'remember', 'that', 'even', 'while', 'i', 'was', 'reading', 'it', 'and', 'although', 'i', 'was', 'only', 'about', '10', 'years', 'old', 'i', 'was', 'aware', 'that', 'the', 'description', 'of', 'the', '``', 'language', '``', 'was', 'not', 'sufficiently', 'detailed', 'in', 'treatment', 'of', 'matters', 'such', 'as', 'consonant', 'clusters', 'or', 'sequences', 'i', \"'\", 'm', 'pretty', 'sure', 'that', '``', 'st', '``', 'would', 'be', 'treated', 'as', 'a', 'cluster', 'with', 'one', '``', 'op', '``', 'inserted', 'after', 'it', 'not', 'an', '``', 'op', '``', 'after', 'the', '``', 's', '``', 'and', 'another', '``', 'op', '``', 'inserted', 'after', 'the', '``', 't', '``', 'thus', '``', 'stay', '``', 'would', 'be', '``', 'stopay', '``', 'not', '``', 'soptopay', '``', 'but', 'i', 'remember', 'that', 'at', 'the', 'time', 'i', 'was', 'aware', 'that', 'i', 'was', 'not', 'sure', 'how', 'such', 'a', 'word', 'would', 'be', 'treated', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'nobuko', 'koyama', '-', 'murakami', ':', 'japanese', 'ba-bi', '-', 'bu-be', '-', 'bo', 'language', 'or', 'lingo', 'was', 'used', 'by', 'teenagers', 'ba', '-', 'bi-bu', '-', 'be-bo', 'language', 'was', 'specifically', 'used', 'when', 'they', 'were', 'teasing', 'or', 'joking', 'with', 'others', 'wishing', 'to', 'make', 'their', 'conversation', 'sound', 'so', 'secretive', 'and', 'purposely', 'annoying', 'others', 'manipulating', 'this', 'language', 'so', 'skillfully', 'was', 'a', 'key', 'to', 'the', 'membership', 'of', 'this', 'group', 'if', 'you', 'mimicked', 'this', 'language', 'poorly', 'you', 'would', 'be', 'automatically', 'excluded', 'from', 'the', 'group', 'one', 'more', 'thing', ':', 'there', 'were', 'some', 'variants', 'in', 'use', 'of', 'this', 'language', 'differences', 'seemed', 'to', 'be', 'strongly', 'related', 'with', 'types', 'of', 'dialects', 'of', 'japanese', 'they', 'spoke', 'in', 'the', 'northern', 'part', 'of', 'the', 'mainland', 'japan', 'ba-bi', '-', 'bu-be', '-', 'bo', 'was', 'inserted', 'accordingly', 'based', 'on', 'phonetics', 'in', 'the', 'tokyo', 'metropolitan', 'areas', 'ba-bi', '-', 'bu-be', '-', 'bo', 'was', 'inserted', 'between', 'orthographic', 'letters', 'at', 'least', 'such', 'was', 'a', 'tendency', 'that', 'i', 'had', 'found', 'e', 'g', '``', 'icecream', '``', 'written', 'as', 'a-i', '-', 'su-ku', '-', 'ri', '-', ':', '-', 'mu', 'nb', '``', ':', '``', 'represents', 'lengthening', 'mark', 'in', 'japanese', 'orthography', 'here', '1', 'a-ba', '-', 'i-bi', '-', 'su-bu', '-', 'ku-bu', '-', 'ri-bi', '-', 'i-bi', '-', 'mu-bu', 'tokyo', '2', 'a-ba', '-', 'i-bi', '-', 'su-bu', '-', 'ku-bu', '-', 'ri-bi', '-', 'i-mu', '-', 'bu', 'as', 'far', 'as', 'i', 'know', 'the', 'age', 'group', 'that', 'i', 'mentioned', 'those', 'were', 'teenagers', 'in', '80', \"'s\", 'were', 'in', 'the', 'rage', 'of', '15-18', 'which', 'means', 'that', 'they', 'were', 'in', 'high', 'school', 'at', 'that', 'time', 'period', ':', 'nb', 'in', 'japan', 'unlike', 'u', 's', 'high', 'school', 'is', 'legally', 'and', 'clearly', 'a', 'separate', 'institute', 'we', 'all', 'encountered', 'and', 'experienced', 'this', 'ba-bi', '-', 'bu-be', '-', 'bo', 'language', 'when', 'we', 'were', 'high', 'school', 'students', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'mark', 'wilson', ':', 'it', \"'s\", 'been', 'several', 'years', 'since', 'i', 'observed', 'the', 'phenomenon', 'i', 'told', 'markel', 'about', 'the', 'german', 'insertion', 'of', '``', 'lav', '``', 'after', 'vowels', 'german', ':', '``', 'lav', '``', 'inserted', 'after', 'vowels', '``', 'ilavich', 'wohlavonelave', 'ilavin', 'balavad', 'holavombulavurg', '``', 'for', '``', 'ich', 'wohne', 'in', 'bad', 'homburg', '``', 'to', 'be', 'more', 'precise', 'the', 'insertion', 'was', '``', 'lavv', '``', 'where', 'v', 'stands', 'for', 'the', 'vowel', 'immediately', 'preceding', 'the', 'inserted', '``', 'l', '``', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3d', '=', '3', 'some', 'tentative', 'conclusions', ':', 'a', 'both', 'phenomena', 'pig', 'latin', '-', 'type', 'phonologically', 'manipulated', 'secret', 'language', 'and', 'youth', 'slang', 'are', 'apparently', 'neither', 'an', 'anglosaxon', 'nor', 'a', 'european', 'particularity', 'b', 'predeliction', 'to', 'pig', 'latin', '-', 'type', 'language', 'game', 'covers', 'a', 'much', 'wider', 'age', 'bracket', 'than', 'i', 'had', 'initially', 'suspected', 'beginning', 'at', 'around', '10', 'years', 'and', 'overlapping', 'with', 'youth', 'slang', 'in', 'which', 'pig', 'latin', '-', 'type', 'expressions', 'may', 'be', 'taken', 'up', 'as', 'slang', '-', 'specific', 'words', 'thanks', 'again', 'to', 'everybody', 'who', 'contributed', 'perhaps', 'i', 'should', 'apologize', 'that', 'this', 'summary', 'got', 'so', 'long', 'but', 'to', 'be', 'honest', 'of', 'course', 'i', 'am', 'very', 'happy', 'to', 'have', 'gotten', 'so', 'much', 'to', 'summarize', 'and', 'thought', 'it', 'would', 'be', 'selfish', 'not', 'to', 'share', 'it', 'with', 'fellow', 'linguist', '-', 'listers', 'and', 'future', 'searchers', 'of', 'the', 'linguist', 'archives', 'for', 'this', 'same', 'reason', 'here', 'are', 'my', 'own', 'experiences', 'with', 'pig', 'latin', ':', 'at', 'age', '12-13', 'years', 'in', 'indonesian', 'junior', 'middle', 'school', 'smp', 'in', 'bogor', 'west', 'java', 'i', 'encountered', 'took', 'part', 'in', 'the', 'following', 'form', 'of', 'pig', 'latin', ':', 'sentences', 'were', 'constructed', 'to', 'preferentially', 'consist', 'of', 'bisyllabic', 'words', 'most', 'basic', 'words', 'in', 'indonesian', 'are', 'bisyllabic', 'and', 'when', 'the', 'first', 'syllable', 'ended', 'in', 'a', 'consonant', 'the', 'entire', 'second', 'syllable', 'was', 'replaced', 'by', 'se', 'e', 'as', 'in', 'english', '``', 'were', '``', 'otherwise', 'the', 'initial', 'consonant', 'of', 'the', 'second', 'syllable', 'was', 'retained', 'and', 'only', 'the', 'rest', 'replaced', ':', 'saya', 'cinta', 'sama', 'kamu', '``', 'i', 'love', 'you', '``', 'c', 'as', 'engl', 'ch', 'became', ':', 'sayse', 'cinse', 'samse', 'kamse', 'from', 'other', 'people', 'i', 'know', 'that', 'similar', 'indonesian', 'pig', 'latins', 'had', 'existed', 'in', 'other', 'parts', 'of', 'indonesia', 'particularly', 'in', 'central', 'and', 'east', 'java', 'most', 'of', 'the', 'ones', 'i', 'heard', 'of', 'had', 'the', 'se', 'insertion', 'but', 'the', 'rules', 'were', 'not', 'always', 'exactly', 'like', 'in', 'bogor', 'in', 'my', 'childhood', 'it', 'was', 'only', 'used', 'occasionally', 'particularly', 'to', 'tease', 'those', 'who', 'were', 'not', '``', 'in', '``', 'to', 'the', 'secret', 'it', 'was', 'a', 'passing', 'fad', 'which', 'lasted', 'not', 'even', 'as', 'long', 'as', 'one', 'school', 'year', 'finally', 'i', 'understand', 'that', 'some', 'time', 'around', '10', 'years', 'ago', 'in', 'israelian', 'pop-music', 'there', 'had', 'been', 'a', 'hit', 'which', 'also', 'became', 'popular', 'outside', 'israel', 'particularly', 'in', 'west', 'europe', 'the', 'title', 'seems', 'to', 'have', 'meant', '``', 'i', 'love', 'you', '``', 'in', 'pig', 'latin', '-', 'style', 'manipulated', 'hebrew', 'can', 'anyone', 'tell', 'me', 'anything', 'of', 'that', 'song', 'but', 'particularly', 'of', 'the', 'hebrew', 'pig', 'latin', 'does', 'anyone', 'know', 'anything', 'about', 'pig', 'latin', 'e', 'g', 'in', 'chinese', 'hindi', 'tamil', 'arabic', 'turkish', 'or', 'suaheli', 'does', 'youth', 'slang', 'exist', 'in', 'amerindian', 'languages', 'in', 'australian', 'aborigine', 'or', 'other', 'languages', 'of', 'pre-industrial', 'communities', 'best', 'regards', 'to', 'all', 'waruno', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'waruno', 'mahdi', 'tel', ':', '+', '49', '30', '8413-5301', 'faradayweg', '4', '-', '6', 'fax', ':', '+', '49', '30', '8413-3155', '14195', 'berlin', 'email', ':', 'mahdi', '@', 'fhi-berlin', 'mpg', 'de', 'germany', 'www', ':', 'http', ':', '/', '/', 'w3', 'rz-berlin', 'mpg', 'de', '/', '~', 'wm', '/', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']), ('spmsgc111', ['Subject', ':', 'is', 'this', 'you', 'to', 'have', 'your', 'name', 'removed', 'from', 'this', 'list', 'call', 'toll', 'free', '1-888', '-', '829-1943', 'an', 'exciting', 'new', 'domain', 'is', 'now', 'available', 'to', 'the', 'public', 'free', 'searches', 'http', ':', '/', '/', 'www', 'webdomains', 'cc', 'with', 'more', 'than', '4', 'million', 'com', 'domains', 'registered', 'all', 'the', 'short', 'and', 'easy-to', '-', 'remember', 'names', 'are', 'gone', 'now', 'a', 'new', 'domain', 'is', 'open', 'to', 'the', 'public', 'and', 'growing', 'very', 'fast', ':', '``', 'cc', '``', 'major', 'companies', 'like', 'intel', 'coca', '-', 'cola', 'and', 'even', 'amazon', 'com', 'have', 'all', 'already', 'registered', 'on', 'the', 'cc', 'domain', 'they', 'protected', 'their', 'domain', 'names', 'protect', 'yours', 'http', ':', '/', '/', 'www', 'webdomains', 'cc', 'the', 'cc', 'domain', 'reintroduces', 'the', 'possibility', 'of', 'owning', 'short', 'easy-to', '-', 'remember', 'domain', 'names', 'and', 'functions', 'exactly', 'as', '``', 'com', '``', 'it', 'is', 'expected', 'to', 'match', '``', 'com', '``', 'in', 'growth', 'and', 'both', 'personal', 'and', 'commercial', 'use', 'worldwide', 'availability', 'of', 'premiere', 'names', 'is', 'excellent', 'don', \"'\", 't', 'wait', 'search', 'for', 'your', 'name', 'for', 'free', 'at', ':', 'http', ':', '/', '/', 'www', 'webdomains', 'cc', 'http', ':', '/', '/', 'www', 'webdomains', 'cc', 'http', ':', '/', '/', 'www', 'webdomains', 'cc', 'do', 'you', 'yahoo', 'get', 'your', 'free', '@', 'yahoo', 'com', 'address', 'at', 'http', ':', '/', '/', 'mail', 'yahoo', 'com'])]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\" Apply the nltk.word_tokenize() method to our text, return the token list. \"\"\"\n",
    "    nltk.download('punkt') #  loads the standard NLTK tokenizer model \n",
    "    # it is important that this is done here in the function, as it needs to be done on every worker.\n",
    "    # If we do the download outside a this function, it would only be executed on the driver     \n",
    "    return nltk.word_tokenize(text)\n",
    "        \n",
    "def removePunctuation(tokens):\n",
    "    \"\"\" Remove punctuation characters from all tokens in a provided list. \"\"\"\n",
    "    # remove all punctuation from string \n",
    "    tokens2 =  [re.sub('[()\\[\\],.?!\";_]','', s) for s in tokens]\n",
    "    return tokens2\n",
    "    \n",
    "def prepareTokenRDD(fn_txt_RDD):\n",
    "    \"\"\" Take an RDD with (filename, text) elements and transform it into a (filename, [token ...]) RDD without punctuation characters. \"\"\"\n",
    "    rdd_vals2 = fn_txt_RDD.values() # It's convenient to process only the values. \n",
    "    rdd_vals3 = rdd_vals2.map(tokenize) # Create a tokenised version of the values by mapping\n",
    "    rdd_vals4 = rdd_vals3.map(removePunctuation) # remove punctuation from the values\n",
    "    rdd_kv = fn_txt_RDD.keys().zip(rdd_vals4) # zip the two RDDs together \n",
    "\n",
    "    # now remove any empty strings created by removing punctuation, and resulting entries without words left.\n",
    "    rdd_kvr = rdd_kv.map(lambda x: (x[0], [s for s in x[1] if len(s) > 0]))\n",
    "    # remove empty token lists \n",
    "    rdd_kvrf = rdd_kvr.filter(lambda x: len(x[1]) > 0)\n",
    "    return rdd_kvrf\n",
    "\n",
    "rdd2 = prepareTokenRDD(rdd1) # Use a small RDD for testing.\n",
    "print(rdd2.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9HgRAEGkeq0"
   },
   "source": [
    "## Create normalised TF.IDF vectors of defined size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Qo_YQKAokeq0",
    "outputId": "1a42d14a-aa91-4ba6-959d-e77e9eb060db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('8-1064msg1', DenseVector([0.6009, 0.1198, 0.1819, 0.0, 0.0, 0.3999, 0.5172, 0.2085, 0.3472, 0.0]))]\n"
     ]
    }
   ],
   "source": [
    "# use hashing to create a fixed-size vector from a word list\n",
    "def hashing_vectorize(text, N): # arguments: the list and the size of the output vector\n",
    "    v = [0] * N  # create vector of 0s\n",
    "    for word in text: # iterate through the words \n",
    "        h = hash(word)\n",
    "        # add 1 at the hashed address \n",
    "        v[h % N] += 1\n",
    "    return v # return hashed word vector\n",
    "\n",
    "from pyspark.mllib.feature import IDF, Normalizer\n",
    "\n",
    "def normTFIDF(fn_tokens_RDD, vecDim):\n",
    "    keysRDD = fn_tokens_RDD.keys()\n",
    "    tokensRDD = fn_tokens_RDD.values()\n",
    "    tfVecRDD = tokensRDD.map(lambda tokens: hashing_vectorize(tokens, vecDim)) \n",
    "    idf = IDF() # create IDF object\n",
    "    idfModel = idf.fit(tfVecRDD) # calculate IDF values\n",
    "    tfIdfRDD = idfModel.transform(tfVecRDD)\n",
    "    norm = Normalizer() # create a Normalizer object\n",
    "    normTfIdfRDD = norm.transform(tfIdfRDD)\n",
    "    zippedRDD = keysRDD.zip(normTfIdfRDD)\n",
    "    return zippedRDD\n",
    "\n",
    "testDim = 10 # too small for good accuracy, but OK for testing\n",
    "rdd3 = normTFIDF(rdd2, testDim) # test normTFIDF function\n",
    "print(rdd3.take(1)) # tuples with ('filename', [N-dim vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f-xzB6hrkeq5"
   },
   "source": [
    "## Create Labelled Points \n",
    "Determine whether the file is spam (i.e. the filename contains ’spmsg’) and replace the filename by a 1 (spam) or 0 (non-spam) accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "wF9BDmnEkeq6",
    "outputId": "8207bd96-452c-41e8-8747-71f5b07d5e82",
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [0.6009150344196178,0.11976690682157425,0.18186826591424235,0.0,0.0,0.39991491844182403,0.5172418880942757,0.2084831340968144,0.3471953532202236,0.0])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# create labelled points of vector size N out of the RDD with normalised (filename, td.idf-vector) items\n",
    "def makeLabeledPoints(fn_vec_RDD):  \n",
    "    # determine the true class as encoded in the filename and represent as 1 (spam) or 0 (good) \n",
    "    cls_vec_RDD = fn_vec_RDD.map(lambda x: (1, x[1]) if x[0].startswith('spmsg') else (0,x[1]))\n",
    "    # create the LabeledPoint objects with (class, vector) arguments\n",
    "    lp_RDD = cls_vec_RDD.map(lambda cls_vec: LabeledPoint(cls_vec[0], cls_vec[1]) ) \n",
    "    return lp_RDD \n",
    "\n",
    "# for testing\n",
    "testLpRDD = makeLabeledPoints(rdd3)\n",
    "print(testLpRDD.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuLuz17zkeq8"
   },
   "source": [
    "## Complete the preprocessing \n",
    "Integrate everything to have a single function to do the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "h9F8Kl3ukeq9",
    "outputId": "9cd6e0bd-4c7f-4cf6-948a-0b874f8960fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [0.2637866600493808,0.44748645332942294,0.6065785052932868,0.0,0.0,0.06051794242403623,0.376799936865379,0.2141387069181108,0.23374073852613333,0.34074074684210165])]\n",
      "creating RDDs\n",
      "/Users/Braganca/lingspam_public/lemm/part3\n",
      "/Users/Braganca/lingspam_public/lemm/part4\n",
      "/Users/Braganca/lingspam_public/lemm/part5\n",
      "/Users/Braganca/lingspam_public/lemm/part2\n",
      "/Users/Braganca/lingspam_public/lemm/part10\n",
      "/Users/Braganca/lingspam_public/lemm/part9\n",
      "/Users/Braganca/lingspam_public/lemm/part7\n",
      "/Users/Braganca/lingspam_public/lemm/part1\n",
      "/Users/Braganca/lingspam_public/lemm/part6\n",
      "/Users/Braganca/lingspam_public/lemm/part8\n",
      "creating RDD union\n",
      "[LabeledPoint(0.0, [0.023025588154406303,0.03990474409476495,0.05543803033005656,0.07555383010020532,0.0,0.03827674183305801,0.37254955965631914,0.030259500929017198,0.047662478924632136,0.0,0.0,0.005040169037386492,0.4236126382119103,0.04430486406570511,0.06061866555976338,0.05871296012132274,0.0,0.045678416985632686,0.035150827322761063,0.03661548480384528,0.01706438371653868,0.06951831832413528,0.0,0.05420450551625396,0.024703742241186762,0.0,0.05543803033005656,0.04197435005566962,0.022127676106416895,0.027409747803815887,0.1770537555809834,0.030585688794482896,0.0,0.0,0.07512809111519808,0.0,0.0,0.058210043826192055,0.056685898777021584,0.05981178058269513,0.03801526071876137,0.030626319787770067,0.063230819058067,0.0,0.20354743199477351,0.02710225275812698,0.20167086430512346,0.03211339407577572,0.026796509830468784,0.021295756517651705,0.21090496393656638,0.0,0.06901317642019185,0.02412050020934194,0.06585254550804942,0.04513590432687893,0.12302789562966372,0.06224467146462205,0.4597741880861797,0.0,0.0,0.0,0.020748223821540682,0.0,0.061252639575540134,0.1231601741094323,0.06792300478872812,0.022972697328412024,0.07857060374416852,0.030626319787770067,0.018975466845233217,0.0840534117602843,0.05118133440993061,0.0842433486445038,0.17257305910576165,0.04651470886870694,0.1147003021511471,0.05189169370797895,0.0,0.08222924341144766,0.018198482644819526,0.0,0.22497982184698698,0.0,0.05900891998632888,0.054819495607631774,0.035665530838206574,0.12777453910591022,0.0,0.07778537312174671,0.0,0.09175706638344869,0.027409747803815887,0.019138370916529004,0.0,0.02389164552479392,0.021571632388220206,0.054819495607631774,0.11010721025663377,0.03827674183305801])]\n",
      "[LabeledPoint(0.0, [0.10056400557345418,0.07858791048501844,0.0,0.021572975402980095,0.08622024797522404,0.128230485491628,0.0,0.16006943138966795,0.1355438542468393,0.03837088264377012,0.2641989335377309,0.369503148837302,0.12463205060434443,0.09612962896713194,0.10715823005458784,0.0,0.02237388097071509,0.01847079139588432,0.04148770519305181,0.10448126244174172,0.08418902086706812,0.03790632377557588,0.049418486950260916,0.0,0.0,0.13920272186458357,0.30216517168571067,0.013800777755562488,0.0,0.21676684767722368,0.07476608124688412,0.14697478446587264,0.07793354336042167,0.05993816853523179,0.03797749435949176,0.03183095475828638,0.0,0.07808018222013816,0.14739662993410596,0.03979602969982856,0.11754142024695943,0.07674176528754025,0.0,0.0663852111205883,0.018106751699767665,0.024699405761837052,0.022590642374473217,0.05224063122087086,0.20596329552436993,0.054543166187136054,0.03695727536660178,0.0,0.05140597029536459,0.12216503735157214,0.10779224122387615,0.02227140431966551,0.11105003731347793,0.046663367189607494,0.08929143582011778,0.019436274173904887,0.10031402553607377,0.07720199076998857,0.023550839553352357,0.09462574653782715,0.1000524272996545,0.03744344707444304,0.03542647200122961,0.03250912510295239,0.0,0.01356754445253711,0.08992360781048067,0.04824646710961661,0.19250609546740696,0.0368192115137971,0.04701392864249229,0.02559331805366339,0.049782953699856365,0.03970380053681024,0.0,0.09061673391229051,0.059864938505378364,0.0339308939121503,0.06933922946164238,0.0,0.01972982641168163,0.02782288875208912,0.04306183235304006,0.09491446687282125,0.2798008150103876,0.04566892850798115,0.0,0.14230979548856257,0.2152477675740607,0.040386253306751926,0.0,0.027166646893861217,0.0,0.1299929234205173,0.0,0.03870516622101349])]\n"
     ]
    }
   ],
   "source": [
    "# apply the preprocessing chain to the data\n",
    "def preprocess(rawRDD, N):\n",
    "    \"\"\" take a (filename,text) RDD and transform into LabelledPoint objects \n",
    "        with class labels and a TF.IDF vector with N dimensions. \n",
    "    \"\"\"\n",
    "    toknRDD = prepareTokenRDD(rawRDD)\n",
    "    normRDD = normTFIDF(toknRDD,N)\n",
    "    lpRDD = makeLabeledPoints(normRDD)\n",
    "    return lpRDD # return RDD with LabeledPoints\n",
    "\n",
    "# this starts the whole process from a directory, N is the vector size\n",
    "def loadAndPreprocess(directory, N):\n",
    "    \"\"\" load lingspam data from a directory and create a training and test set of preprocessed data \"\"\"\n",
    "    ftrainRDD_testRDD = makeTestTrainRDDs(directory)\n",
    "    (trainRDD,testRDD) = ftrainRDD_testRDD\n",
    "    return (preprocess(trainRDD, N), preprocess(testRDD, N)) # apply the preprocessing function defined above\n",
    "\n",
    "trainLpRDD = preprocess(trainRDD, testDim) # prepare the training data\n",
    "print(trainLpRDD.take(1)) # check\n",
    "\n",
    "train_test_LpRDD = loadAndPreprocess('lemm', 100)\n",
    "(trainLpRDD, testLpRDD) = train_test_LpRDD\n",
    "print(testLpRDD.take(1))\n",
    "print(trainLpRDD.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdwyXqu4keq_"
   },
   "source": [
    "## Train some classifiers \n",
    "\n",
    "Use the `LabeledPoint` objects to train a classifier, specifically *Logistic Regression*, *Naive Bayes*, and *Support Vector Machine* and calculate the accuracy of the model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "Ljcgl0mLkeq_",
    "outputId": "cbe52324-9866-4455-d44c-a6a3c8295317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8337173579109063"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, LogisticRegressionWithLBFGS, SVMWithSGD\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "def trainModel(lpRDD):\n",
    "    \"\"\" Train 3 classifier models on the given RDD with LabeledPoint objects.\"\"\"\n",
    "    print('Starting to train the model')\n",
    "    model1 = LogisticRegressionWithLBFGS.train(lpRDD)\n",
    "    print('Trained LR (model1)')\n",
    "    model2 = NaiveBayes.train(lpRDD)\n",
    "    print('Trained NB (model2)')\n",
    "    model3 = SVMWithSGD.train(lpRDD)\n",
    "    print('Trained SVM (model3)')\n",
    "    return [model1, model2, model3]\n",
    "\n",
    "def testModel(model, lpRDD):\n",
    "    \"\"\" Tests the classification accuracy of the given model on the given RDD with LabeledPoint objects. \"\"\"\n",
    "    lpRDD.persist(StorageLevel.MEMORY_ONLY)\n",
    "    # Get the prediction and the ground truth label\n",
    "    predictionAndLabel = lpRDD.map(lambda p: (model.predict(p.features), p.label))\n",
    "    correct = predictionAndLabel.filter(lambda xv: xv[0] == xv[1]).count() \n",
    "    accuracy = correct/lpRDD.count()\n",
    "    print('Accuracy {:.1%} (data items: {}, correct: {})'.format(accuracy, lpRDD.count(), correct))\n",
    "    return accuracy\n",
    "\n",
    "models = trainModel(trainLpRDD) # just for testing\n",
    "testModel(models[2], trainLpRDD) # just for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TeHFXcTkerC"
   },
   "source": [
    "## Automate training and testing\n",
    "\n",
    "Automate the whole process from reading the files, through preprocessing, and training up to evaluating the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "Kj_Xh5oekerD",
    "outputId": "2de3d889-a491-4bc6-bcbd-104c7b152207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading and preprocessing\n",
      "creating RDDs\n",
      "/Users/Braganca/lingspam_public/lemm/part3\n",
      "/Users/Braganca/lingspam_public/lemm/part4\n",
      "/Users/Braganca/lingspam_public/lemm/part5\n",
      "/Users/Braganca/lingspam_public/lemm/part2\n",
      "/Users/Braganca/lingspam_public/lemm/part10\n",
      "/Users/Braganca/lingspam_public/lemm/part9\n",
      "/Users/Braganca/lingspam_public/lemm/part7\n",
      "/Users/Braganca/lingspam_public/lemm/part1\n",
      "/Users/Braganca/lingspam_public/lemm/part6\n",
      "/Users/Braganca/lingspam_public/lemm/part8\n",
      "creating RDD union\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
      "Testing\n",
      "Accuracy 97.6% (data items: 289, correct: 282)\n",
      "Training\n",
      "Accuracy 94.6% (data items: 2604, correct: 2464)\n",
      "Testing\n",
      "Accuracy 90.0% (data items: 289, correct: 260)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.946236559139785, 0.8337173579109063],\n",
       " [0.9757785467128027, 0.8996539792387543, 0.8339100346020761]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trainTestModel(trainRDD, testRDD):\n",
    "    \"\"\" Trains 3 models and tests them on training and test data. Returns a matrix with the training and testing (rows) accuracy values for all models (columns). \"\"\"\n",
    "    models = trainModel(trainRDD)\n",
    "    results = [[], []] # training, test for the models\n",
    "    for mdl in models:\n",
    "        print('Training')\n",
    "        results[0].append(testModel(mdl, trainRDD))\n",
    "        print('Testing')\n",
    "        results[1].append(testModel(mdl, testRDD))\n",
    "    return results\n",
    "\n",
    "def trainTestFolder(folder,N):\n",
    "    \"\"\" Reads data from a folder, preproceses the data, and trains and evaluates models on it. \"\"\"\n",
    "    print('Start loading and preprocessing') \n",
    "    train_test_LpRDD = loadAndPreprocess(folder,N) # create the RDDs\n",
    "    print('Finished loading and preprocessing')\n",
    "    (trainLpRDD, testLpRDD) = train_test_LpRDD # unpack the RDDs \n",
    "    return trainTestModel(trainLpRDD,testLpRDD) # train and test\n",
    "\n",
    "trainTestFolder('lemm', 1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzcNex1NkerF"
   },
   "source": [
    "## Run experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HmW_CpetkerG",
    "outputId": "f74519df-5e4b-4ecb-fdd8-cf6dd67d1037",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 1: Testing different vector sizes\n",
      "\n",
      "N = 3\n",
      "Start loading and preprocessing\n",
      "creating RDDs\n",
      "/Users/Braganca/lingspam_public/bare/part3\n",
      "/Users/Braganca/lingspam_public/bare/part4\n",
      "/Users/Braganca/lingspam_public/bare/part5\n",
      "/Users/Braganca/lingspam_public/bare/part2\n",
      "/Users/Braganca/lingspam_public/bare/part10\n",
      "/Users/Braganca/lingspam_public/bare/part9\n",
      "/Users/Braganca/lingspam_public/bare/part7\n",
      "/Users/Braganca/lingspam_public/bare/part1\n",
      "/Users/Braganca/lingspam_public/bare/part6\n",
      "/Users/Braganca/lingspam_public/bare/part8\n",
      "creating RDD union\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "\n",
      "N = 30\n",
      "Start loading and preprocessing\n",
      "creating RDDs\n",
      "/Users/Braganca/lingspam_public/bare/part3\n",
      "/Users/Braganca/lingspam_public/bare/part4\n",
      "/Users/Braganca/lingspam_public/bare/part5\n",
      "/Users/Braganca/lingspam_public/bare/part2\n",
      "/Users/Braganca/lingspam_public/bare/part10\n",
      "/Users/Braganca/lingspam_public/bare/part9\n",
      "/Users/Braganca/lingspam_public/bare/part7\n",
      "/Users/Braganca/lingspam_public/bare/part1\n",
      "/Users/Braganca/lingspam_public/bare/part6\n",
      "/Users/Braganca/lingspam_public/bare/part8\n",
      "creating RDD union\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 86.0% (data items: 2604, correct: 2239)\n",
      "Testing\n",
      "Accuracy 85.5% (data items: 289, correct: 247)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "\n",
      "N = 300\n",
      "Start loading and preprocessing\n",
      "creating RDDs\n",
      "/Users/Braganca/lingspam_public/bare/part3\n",
      "/Users/Braganca/lingspam_public/bare/part4\n",
      "/Users/Braganca/lingspam_public/bare/part5\n",
      "/Users/Braganca/lingspam_public/bare/part2\n",
      "/Users/Braganca/lingspam_public/bare/part10\n",
      "/Users/Braganca/lingspam_public/bare/part9\n",
      "/Users/Braganca/lingspam_public/bare/part7\n",
      "/Users/Braganca/lingspam_public/bare/part1\n",
      "/Users/Braganca/lingspam_public/bare/part6\n",
      "/Users/Braganca/lingspam_public/bare/part8\n",
      "creating RDD union\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
      "Testing\n",
      "Accuracy 92.0% (data items: 289, correct: 266)\n",
      "Training\n",
      "Accuracy 86.2% (data items: 2604, correct: 2245)\n",
      "Testing\n",
      "Accuracy 84.8% (data items: 289, correct: 245)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "\n",
      "N = 3000\n",
      "Start loading and preprocessing\n",
      "creating RDDs\n",
      "/Users/Braganca/lingspam_public/bare/part3\n",
      "/Users/Braganca/lingspam_public/bare/part4\n",
      "/Users/Braganca/lingspam_public/bare/part5\n",
      "/Users/Braganca/lingspam_public/bare/part2\n",
      "/Users/Braganca/lingspam_public/bare/part10\n",
      "/Users/Braganca/lingspam_public/bare/part9\n",
      "/Users/Braganca/lingspam_public/bare/part7\n",
      "/Users/Braganca/lingspam_public/bare/part1\n",
      "/Users/Braganca/lingspam_public/bare/part6\n",
      "/Users/Braganca/lingspam_public/bare/part8\n",
      "creating RDD union\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
      "Testing\n",
      "Accuracy 96.9% (data items: 289, correct: 280)\n",
      "Training\n",
      "Accuracy 97.6% (data items: 2604, correct: 2541)\n",
      "Testing\n",
      "Accuracy 94.5% (data items: 289, correct: 273)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "\n",
      "N = 30000\n",
      "Start loading and preprocessing\n",
      "creating RDDs\n",
      "/Users/Braganca/lingspam_public/bare/part3\n",
      "/Users/Braganca/lingspam_public/bare/part4\n",
      "/Users/Braganca/lingspam_public/bare/part5\n",
      "/Users/Braganca/lingspam_public/bare/part2\n",
      "/Users/Braganca/lingspam_public/bare/part10\n",
      "/Users/Braganca/lingspam_public/bare/part9\n",
      "/Users/Braganca/lingspam_public/bare/part7\n",
      "/Users/Braganca/lingspam_public/bare/part1\n",
      "/Users/Braganca/lingspam_public/bare/part6\n",
      "/Users/Braganca/lingspam_public/bare/part8\n",
      "creating RDD union\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
      "Testing\n",
      "Accuracy 94.1% (data items: 289, correct: 272)\n",
      "Training\n",
      "Accuracy 86.8% (data items: 2604, correct: 2259)\n",
      "Testing\n",
      "Accuracy 83.7% (data items: 289, correct: 242)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2172)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n"
     ]
    }
   ],
   "source": [
    "folder = 'bare'\n",
    "vectorsizes = [3, 30, 300, 3000, 30000]\n",
    "print('EXPERIMENT 1: Testing different vector sizes')\n",
    "results_vectorsizes = []\n",
    "for n in vectorsizes:\n",
    "    print('\\nN = {}'.format(n))\n",
    "    result = {'n': n, 't': folder}\n",
    "    result['acc'] = trainTestFolder(folder, n)\n",
    "    results_vectorsizes.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Y3ll1Ag9BLqb",
    "outputId": "4124bd14-d10d-4981-e477-e4c964621024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 2: Testing different data types\n",
      "\n",
      "Path = bare\n",
      "Start loading and preprocessing\n",
      "creating RDDs\n",
      "/Users/Braganca/lingspam_public/bare/part3\n",
      "/Users/Braganca/lingspam_public/bare/part4\n",
      "/Users/Braganca/lingspam_public/bare/part5\n",
      "/Users/Braganca/lingspam_public/bare/part2\n",
      "/Users/Braganca/lingspam_public/bare/part10\n",
      "/Users/Braganca/lingspam_public/bare/part9\n",
      "/Users/Braganca/lingspam_public/bare/part7\n",
      "/Users/Braganca/lingspam_public/bare/part1\n",
      "/Users/Braganca/lingspam_public/bare/part6\n",
      "/Users/Braganca/lingspam_public/bare/part8\n",
      "creating RDD union\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
      "Testing\n",
      "Accuracy 96.9% (data items: 289, correct: 280)\n",
      "Training\n",
      "Accuracy 97.6% (data items: 2604, correct: 2541)\n",
      "Testing\n",
      "Accuracy 94.5% (data items: 289, correct: 273)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "\n",
      "Path = stop\n",
      "Start loading and preprocessing\n",
      "creating RDDs\n",
      "/Users/Braganca/lingspam_public/stop/part3\n",
      "/Users/Braganca/lingspam_public/stop/part4\n",
      "/Users/Braganca/lingspam_public/stop/part5\n",
      "/Users/Braganca/lingspam_public/stop/part2\n",
      "/Users/Braganca/lingspam_public/stop/part10\n",
      "/Users/Braganca/lingspam_public/stop/part9\n",
      "/Users/Braganca/lingspam_public/stop/part7\n",
      "/Users/Braganca/lingspam_public/stop/part1\n",
      "/Users/Braganca/lingspam_public/stop/part6\n",
      "/Users/Braganca/lingspam_public/stop/part8\n",
      "creating RDD union\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
      "Testing\n",
      "Accuracy 96.9% (data items: 289, correct: 280)\n",
      "Training\n",
      "Accuracy 96.9% (data items: 2604, correct: 2524)\n",
      "Testing\n",
      "Accuracy 93.4% (data items: 289, correct: 270)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "\n",
      "Path = lemm\n",
      "Start loading and preprocessing\n",
      "creating RDDs\n",
      "/Users/Braganca/lingspam_public/lemm/part3\n",
      "/Users/Braganca/lingspam_public/lemm/part4\n",
      "/Users/Braganca/lingspam_public/lemm/part5\n",
      "/Users/Braganca/lingspam_public/lemm/part2\n",
      "/Users/Braganca/lingspam_public/lemm/part10\n",
      "/Users/Braganca/lingspam_public/lemm/part9\n",
      "/Users/Braganca/lingspam_public/lemm/part7\n",
      "/Users/Braganca/lingspam_public/lemm/part1\n",
      "/Users/Braganca/lingspam_public/lemm/part6\n",
      "/Users/Braganca/lingspam_public/lemm/part8\n",
      "creating RDD union\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
      "Testing\n",
      "Accuracy 97.6% (data items: 289, correct: 282)\n",
      "Training\n",
      "Accuracy 97.6% (data items: 2604, correct: 2541)\n",
      "Testing\n",
      "Accuracy 93.8% (data items: 289, correct: 271)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n",
      "\n",
      "Path = lemm_stop\n",
      "Start loading and preprocessing\n",
      "creating RDDs\n",
      "/Users/Braganca/lingspam_public/lemm_stop/part3\n",
      "/Users/Braganca/lingspam_public/lemm_stop/part4\n",
      "/Users/Braganca/lingspam_public/lemm_stop/part5\n",
      "/Users/Braganca/lingspam_public/lemm_stop/part2\n",
      "/Users/Braganca/lingspam_public/lemm_stop/part10\n",
      "/Users/Braganca/lingspam_public/lemm_stop/part9\n",
      "/Users/Braganca/lingspam_public/lemm_stop/part7\n",
      "/Users/Braganca/lingspam_public/lemm_stop/part1\n",
      "/Users/Braganca/lingspam_public/lemm_stop/part6\n",
      "/Users/Braganca/lingspam_public/lemm_stop/part8\n",
      "creating RDD union\n",
      "Finished loading and preprocessing\n",
      "Starting to train the model\n",
      "Trained LR (model1)\n",
      "Trained NB (model2)\n",
      "Trained SVM (model3)\n",
      "Training\n",
      "Accuracy 100.0% (data items: 2604, correct: 2604)\n",
      "Testing\n",
      "Accuracy 97.9% (data items: 289, correct: 283)\n",
      "Training\n",
      "Accuracy 96.9% (data items: 2604, correct: 2524)\n",
      "Testing\n",
      "Accuracy 93.4% (data items: 289, correct: 270)\n",
      "Training\n",
      "Accuracy 83.4% (data items: 2604, correct: 2171)\n",
      "Testing\n",
      "Accuracy 83.4% (data items: 289, correct: 241)\n"
     ]
    }
   ],
   "source": [
    "n = 3000\n",
    "typeFolders = ['bare', 'stop', 'lemm', 'lemm_stop']\n",
    "print('EXPERIMENT 2: Testing different data types')\n",
    "results_preprocessing = []\n",
    "for folder in typeFolders:\n",
    "    print('\\nPath = {}'.format(folder))\n",
    "    result = {'n': n, 't': folder}\n",
    "    result['acc'] = trainTestFolder(folder, n)\n",
    "    results_preprocessing.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9n4ecn_Z3_k"
   },
   "source": [
    "Comment:\n",
    "\n",
    "Normally we would expect a very high performance of the classifiers when tested on the training data and naturally lower performance on the testing data. But we see that there is no noticeable difference in the performance of the classifiers when tested using the training(83.4%) and testing(83.4%) sets, which would mean that the classifiers are not effective in distinguishing between the two classes and classify every data point as non-spam(majority class). This is for the case when we use lower values of Vector Size 'N' (3, 30). However, for higher values of 'N' (300+) two of the three models achieve much better performance on the training set with Logistic regression(100%) and Naive Bayes(97.4%) with no improvement in performance for the SVM model across all the cases of values of N.\n",
    "\n",
    "This improvement in performance can be attributed to the size of the hash vector where a larger size allows for fewer collisions and thus accomodation of more words and consequently helps the classifier in learning to better distinguish between the classes, compared to a smaller vector size(N) which results in higher collisions between words and thus a loss of information resulting in lower performance as the data to train the classifier is not as effective.\n",
    "\n",
    "Another important aspect is the corpus that has been used for the training of the models. A corpus which has not had any pre processing done on it should generally result in lower accuracy values, but in our case the Naive Bayes model achieves a higher accuracy while testing using the test data on the \"bare\" corpus as compared to the other corpuses and similar results for the Logistic regression model. This might be indicative of other factors that need inspection which might have an effect on the specific chosen models (class imbalance in the data set, ineffective preprocessing).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Coursework-Part1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
